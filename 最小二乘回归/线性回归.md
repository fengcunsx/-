# 线性回归

**多维降维到一维**

## 一.最下二乘法的定义：

### 定义：

#### 自变量：

$$
X=\begin{pmatrix}x_1,x_2,x_3,x_4,x_5,.....,x_n\end{pmatrix}^T=\begin{pmatrix}x_{11},x_{12},....,x_{1p}\\x_{21},x_{22},....,x_{2p}\\.....\\.....\\x_{n1},x_{n2},....,x_{np}\\\end{pmatrix}
$$

X是一个P维的空间
$$
x_i=\begin{pmatrix}x_{i1}\\x_{i2}\\.\\.\\.\\.x_{ip}\end{pmatrix}
$$

$$
X^T=\begin{pmatrix}x_{11},x_{11},....,x_{1n}\\x_{21},x_{22},....,x_{2n}\\.....\\.....\\x_{p1},x_{pn},....,x_{pn}\end{pmatrix}
$$

注：
$$
x_{ij}表示i行j列
$$

#### 因变量：

$$
Y=\begin{pmatrix}y_1\\y_2\\.\\.\\.\\.\\y_n\end{pmatrix}
$$

那么


$$
Y=f(X)=W^TX=X^T\Beta Y=f(X)=W^TX=X^T\Beta\\
其中\quad W=(X^TX)^{-1}X^TY是最优的参数
$$


### 证明最小二乘法



#### 角度1(分着考虑）：

$$
Y=f(X)=W^TX
$$

![image-20200519170246782](C:\Users\WMK\AppData\Roaming\Typora\typora-user-images\image-20200519170246782.png)

误差：
$$
L(W)=\sum_{i=1}^n\|w^Tx_i-y_i\|^2\\
=(W^TX^T-Y^T)\cdot(W^TX^T-Y^T)^T
\\=(W^TX^T-Y^T)\cdot(XW-Y)\\
=W^TX^TXW-2W^TX^TY+Y^TY
$$
求导得到：
$$
W=argminL(X)
$$

$$
\frac{\sigma L(x)}{W}=0\\=2X^TXW-2X^TY
$$



即：
$$
XX^TW=YX^T\\
W=(X^TX)^{-1}X^TY
$$

#### 角度2（整体考虑）：

$$
Y=f(X)=x^T\Beta
$$

$$
\vec{a}\bot X事误差最小（我真的不知道怎么来的）
$$

![image-20200519165636551](C:\Users\WMK\AppData\Roaming\Typora\typora-user-images\image-20200519165636551.png)
$$
\vec{a}=(y-x\Beta)\\
X\bot\vec{a}\quad\\
$$
那么：
$$
X^T\cdot\vec{a}=X^T(Y-X\Beta)=\vec{0} 
$$
即是：
$$
\Beta=(X^TX)^{-1}X^TY
$$

## 二.概率，极大似然估计角度看

### 已知：

$$
\varepsilon-N(0,\sigma^2)\\
y=wx+\varepsilon
$$

那么：
$$
E(y)=w^Tx\\
Var(y)=\sigma^2\\
y-N(w^Tx,\sigma^2)\\
P(y|w,x)=\frac{1}{\sqrt{2\pi}\cdot\sigma}exp(-\frac{(y-w^Tx)^2}{2\sigma^2})
$$


### 推论：

（利用极大似然估计）
$$
log \quad LikeHood
=ln\prod_{i=1}^nP(y_i|w,x_i)
\\=\sum_{i=1}^nlnP(y_i|w,x_i)\\
=n\cdot ln\frac{1}{\sqrt{2\pi}\cdot\sigma}-\sum_{i=1}^n\frac{(y-w^Tx)^2}{2\sigma^2}
$$
那么极大似然估计：
$$
W=argmax_wlog\quad LikeHood
$$
化简
$$
\frac{d\quad logLikeHood}{d\quad w}=0\\
$$
即在
$$
W=argmin_w\sum_{i=1}^n(w^Tx-y)\quad时取得最大值
$$
**LSE=MLE(误差是高斯分布时)**

## 三.处理过拟合

$$
方法\begin{cases}
增加数据\\
特征选取，选取一定的维度\\
正则化（下边介绍这种方法）
\end{cases}
$$

### 正则化：

误差加上一个惩罚项。

误差改写为：
$$
L(w)=(l(w)+\lambda p(w))
$$
其中，p(w)通常取w的一范数，二范数。下边对二范数的情况进行详细的解释。

### 取二范数时P(w)=||W||_2

$$
L(w)=(l(w)+\lambda p(w))\\
=\sum_{i=1}^n\|w^Tx_i-y_i\|+\lambda\|w\|_2
\\=(W^TX^T-Y^T)\cdot(XW-Y)+\lambda W^TW\\
=W^TX^TXW-2W^TX^TY+Y^TY+\lambda W^TW\\
=W^T(X^TX-\lambda I)W-2W^TX^TY+Y^TY
$$

进行极大似然估计
$$
w=argmin_wL(w)
$$
求导:
$$
\frac{dL(w)}{dw}=2(X^TX-\lambda I)-2X^TY=0
$$
得到结果
$$
w=(X^TX-\lambda I)^{-1}X^TY
$$

## 四.贝叶斯角度看

$$
P(w|y)=\frac{P(y|w)\cdot P(w)}{P(w)}
$$

通过这个公式推导线性回归的式子：

### 已知：

**线性回归噪声为高斯分布**
$$
\varepsilon-N(0,\sigma^2)\\
y=wx+\varepsilon
$$

那么：
$$
E(y)=w^Tx\\
Var(y)=\sigma^2\\
y-N(w^Tx,\sigma^2)\\
P(y|x,w)=\frac{1}{\sqrt{2\pi}\cdot\sigma}exp(-\frac{(y-w^Tx)^2}{2\sigma^2})
$$

同时x先验分布:
$$
P(w)-（0，\sigma_0^2）
$$
那么：
$$
w^{.}=argmax\quad P(w|y)\\=argmax\quad P(y|w)\cdot P(w)\\
=argmax\quad lnP(y|w)\cdot P(w)\\
=argmax\quad -\frac{(y-w^Tx)^2}{2\sigma^2} -\frac{w^2}{2\sigma_0^2}\\
=argmin\quad \frac{(y-w^Tx)^2}{2\sigma^2} +\frac{w^2}{2\sigma_0^2}\\
=argmin\quad (y-w^Tx)^2 -\frac{2\sigma^2}{2\sigma_0^2}\|w\|_2
$$
这个就是正则化的线性回归式子。

**噪声为高斯分布的贝叶斯分布就是线性回归**

## 五，加权最小二乘法

损失函数：$L(\theta)=(y-X\theta)^TW(y-X\theta)$
$$
L(\theta)=(y-X\theta)^TW(y-X\theta)\quad 展开可得:\\
=y^TWy+\theta^TX^TWX\theta-2\theta^TX^Ty\quad 对\theta求导可得：\\
2\theta X^TWX-2X^TWY=0\quad 移项变换可得：\\
\theta=(X^TWX)^{-1}X^TWY
$$

## 六，岭回归

岭回归在损失函数上边加上一个二范数，进而可以保证$X^TX+\lambda I$可逆非奇异

损失函数：$L(\theta)=(y-X\theta)^T(y-X\theta)+\lambda||\theta||_2$
$$
L(\theta)=(y-X\theta)^T(y-X\theta)+\lambda||\theta||_2\quad 展开可得：\\
=y^Ty-2\theta^T X^T+\theta^TX^TX\theta+\lambda\theta^T\theta\quad 求导可得：\\
=-2X^T+2\theta X^TX+2\lambda I\quad 即：\\
\theta=(X^TX+\lambda\cdot I)^{-1}\cdot X^TY
$$

## 七，Lasso回归

lasso回归是在损失函数上边加上一个一范数，进而可以得到对结果y影响大的变量。

损失函数：$L(\theta)=||Y-X\theta||_2+\lambda|\theta|$

目标：
$$
\theta=argmin_{\theta}||Y-X\theta||_2+\lambda|z|\\
s.t.\quad \theta=z\\
式（7.1）
$$
因为损失函数含有绝对值$|\theta|$，所以说我们求导的话就要分类讨论，所以说我们一般采用迭代的方法进行求解。

求解一般采用admm算法。

增广拉格朗日乘数：
$$
L(x)=||Y-X\theta||_2+\lambda|z|+u^T(\theta-z)+\frac{1}2{}||\theta-z||_2
$$

$$
\theta^{k+1}=(X^TX-I)^{-1}(X^TY-u^k+z^k)\\
z=\cases{
\theta^{k+1}-u-\lambda\quad,\theta-u>=\lambda\\
\theta^{k+1}-u+\lambda\quad,\theta-u<=-\lambda\\
0\quad,-\lambda<\theta-u<\lambda
}
$$


## 八，损失函数选择一范数和二范数的区别

### 一范数：

我们可以避免偏离点对数据的影响，我们得到的是中位数

损失函数：$L(x)=|Y-X^T\theta|_1$

### 二范数：

我们得到的是平均数

## 拓展的损失函数：

### huber

![](C:\Users\WMK\Desktop\ML\图像\huber.png)

### tukey

![](C:\Users\WMK\Desktop\ML\图像\tukey.png)

